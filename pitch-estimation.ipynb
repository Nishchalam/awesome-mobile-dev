{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12349725,"sourceType":"datasetVersion","datasetId":7779672},{"sourceId":12350226,"sourceType":"datasetVersion","datasetId":7785978},{"sourceId":12369490,"sourceType":"datasetVersion","datasetId":7799056}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nishchalamukku/pitch-estimation?scriptVersionId=248975508\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nfrom torch import nn \nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport re\nfrom torch.nn.utils.rnn import pad_sequence\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom librosa.filters import mel\nfrom librosa.util import pad_center\nfrom scipy.signal import get_window\nimport os","metadata":{"_uuid":"8aa31323-6a17-4607-8e38-2c8a3066b13f","_cell_guid":"7cc6d6c0-c82b-49f8-83c8-35c041a45581","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:57:14.918135Z","iopub.execute_input":"2025-07-05T16:57:14.91858Z","iopub.status.idle":"2025-07-05T16:57:33.109736Z","shell.execute_reply.started":"2025-07-05T16:57:14.918561Z","shell.execute_reply":"2025-07-05T16:57:33.108939Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"PAD_IDX=0\nBATCH_SIZE = 16\nN_CLASS=360","metadata":{"_uuid":"f8279f15-e405-4fac-aee4-fdd9025f660e","_cell_guid":"8adbc028-fcc7-4787-b867-faa7fcd676c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-05T16:58:51.345135Z","iopub.execute_input":"2025-07-05T16:58:51.345404Z","iopub.status.idle":"2025-07-05T16:58:51.349031Z","shell.execute_reply.started":"2025-07-05T16:58:51.345387Z","shell.execute_reply":"2025-07-05T16:58:51.348264Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"TRAINING_DEVICE = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#TRAINING_DEVICE = \"cpu\"\nprint(f\"Using {TRAINING_DEVICE} device\")","metadata":{"_uuid":"33467b0a-e068-4513-b465-9b6323e619da","_cell_guid":"49873f98-0726-4200-a734-58391d2b934f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:57:33.115855Z","iopub.execute_input":"2025-07-05T16:57:33.116095Z","iopub.status.idle":"2025-07-05T16:57:33.185186Z","shell.execute_reply.started":"2025-07-05T16:57:33.116079Z","shell.execute_reply":"2025-07-05T16:57:33.184424Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# data_path = \"/speech/nishanth/clean_research/ptdb_full_data/mudit_full_final_data_1.008\"\ndata_path=\"/kaggle/input/pitch-data/data_1.008\"","metadata":{"_uuid":"349dff93-b020-48fa-b13b-e01371e95569","_cell_guid":"c023d931-acf0-4f41-8fd1-cd2c0f039788","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:57:39.82879Z","iopub.execute_input":"2025-07-05T16:57:39.829388Z","iopub.status.idle":"2025-07-05T16:57:39.832807Z","shell.execute_reply.started":"2025-07-05T16:57:39.829363Z","shell.execute_reply":"2025-07-05T16:57:39.831958Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_X = os.listdir(f\"{data_path}/train/gd_1.008\")\ntrain_X = [os.path.join(f\"{data_path}/train/gd_1.008\", x) for x in train_X]\ntrain_y = os.listdir(f\"{data_path}/train/labels\")\ntrain_y = [os.path.join(f\"{data_path}/train/labels\", y) for y in train_y]\n\nvalid_X = os.listdir(f\"{data_path}/valid/gd_1.008\")\nvalid_X = [os.path.join(f\"{data_path}/valid/gd_1.008\", x) for x in valid_X]\nvalid_y = os.listdir(f\"{data_path}/valid/labels\")\nvalid_y = [os.path.join(f\"{data_path}/valid/labels\", y) for y in valid_y]","metadata":{"_uuid":"7ef8df46-6b14-4ba8-bbd8-1b07ee2177d7","_cell_guid":"bbfdd34f-392e-44f6-83c1-c3783912e068","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:57:42.006414Z","iopub.execute_input":"2025-07-05T16:57:42.006963Z","iopub.status.idle":"2025-07-05T16:57:42.41508Z","shell.execute_reply.started":"2025-07-05T16:57:42.006933Z","shell.execute_reply":"2025-07-05T16:57:42.414325Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_data = list(zip(train_X, train_y))\nvalid_data = list(zip(valid_X, valid_y))","metadata":{"_uuid":"c2ccb481-7261-4541-9e77-e2984099129a","_cell_guid":"69956bc5-6348-4d75-91dd-e52204de388a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:57:44.734205Z","iopub.execute_input":"2025-07-05T16:57:44.734731Z","iopub.status.idle":"2025-07-05T16:57:44.742971Z","shell.execute_reply.started":"2025-07-05T16:57:44.734708Z","shell.execute_reply":"2025-07-05T16:57:44.742221Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class ModelCheckpointStore:\n    def __init__(self, dump_dir):\n        self.dump_dir = dump_dir\n        self.best_param_epoch = None\n        self.last_save_step = 0\n\n    def __call__(self, model, training_metrics, metric, current_step):\n        param = \"acc\" if metric.endswith(\"acc\") else \"loss\"\n        model_save_dir = os.path.join(self.dump_dir, \"checkpoints\")\n        if not os.path.exists(model_save_dir):\n            os.makedirs(model_save_dir)\n\n        new_valid = os.path.join(model_save_dir, f\"step_{current_step}_{param}_latest.pth\")\n        old_valid = os.path.join(model_save_dir, f\"step_{self.last_save_step}_{param}_latest.pth\")\n        torch.save(model, new_valid)\n        self.last_save_step = current_step\n\n        if len(training_metrics[metric]) == 1:\n            best_checkpoint = os.path.join(model_save_dir, f\"step_{current_step}_{param}_best.pth\")\n            torch.save(model.state_dict(), best_checkpoint)\n            self.best_param_epoch = current_step\n        else:\n            is_best = False\n            if param == \"acc\" and training_metrics[metric][-1] > training_metrics[metric][-2]:\n                is_best = True\n            elif param == \"loss\" and training_metrics[metric][-1] < training_metrics[metric][-2]:\n                is_best = True\n\n            if is_best:\n                best_checkpoint = os.path.join(model_save_dir, f\"step_{current_step}_{param}_best.pth\")\n                torch.save(model, best_checkpoint)\n                old_best = os.path.join(model_save_dir, f\"step_{self.best_param_epoch}_{param}_best.pth\")\n                if os.path.exists(old_best):\n                    os.remove(old_best)\n                self.best_param_epoch = current_step","metadata":{"_uuid":"8c6a5dbe-f00c-4d5a-883d-0af5cb947e1f","_cell_guid":"250abc19-39d3-4162-9bb3-78fe9d95bc5e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:57:46.067013Z","iopub.execute_input":"2025-07-05T16:57:46.06754Z","iopub.status.idle":"2025-07-05T16:57:46.074803Z","shell.execute_reply.started":"2025-07-05T16:57:46.067519Z","shell.execute_reply":"2025-07-05T16:57:46.074059Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def display(step, training_metrics):\n    to_print = f\"Epoch : {step}\\n\" + \"\\n\".join([f\"{key} : {value[-1]}\" for key, value in training_metrics.items()])        \n    print(to_print)","metadata":{"_uuid":"2d5a747e-2c21-4935-a059-995b2783350c","_cell_guid":"f2968f68-d290-4677-846d-75c20e5f3f14","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:57:49.266242Z","iopub.execute_input":"2025-07-05T16:57:49.266877Z","iopub.status.idle":"2025-07-05T16:57:49.270483Z","shell.execute_reply.started":"2025-07-05T16:57:49.266852Z","shell.execute_reply":"2025-07-05T16:57:49.269946Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def plot(dump_dir, model_name, **kwargs):\n    plot_save_path = os.path.join(dump_dir, f\"{model_name}_plots\")\n    if not os.path.exists(plot_save_path):\n        os.makedirs(plot_save_path)\n    acc_plot = plt.figure(figsize=(15, 8))\n    loss_plot = plt.figure(figsize=(15, 8))\n\n    for key, value in kwargs.items():\n        if key.endswith('acc'):\n            plt.figure(acc_plot.number)\n            plt.plot(value, label=key)\n            plt.xlabel('steps')\n            plt.ylabel('Accuracy')\n            plt.legend()\n            plt.title('Accuracy')\n\n        elif key.endswith('loss'):\n            plt.figure(loss_plot.number)\n            plt.plot(value, label=key)\n            plt.xlabel('steps')\n            plt.ylabel('Loss')\n            plt.legend()\n            plt.title('Loss')\n\n    plt.figure(acc_plot.number)\n    plt.savefig(os.path.join(plot_save_path, 'acc.png'))\n    plt.close(acc_plot)\n\n    plt.figure(loss_plot.number)\n    plt.savefig(os.path.join(plot_save_path, 'loss.png'))\n    plt.close(loss_plot)","metadata":{"_uuid":"2b97203e-4f78-49c7-ba75-554efdf6f09e","_cell_guid":"eb0d9b51-694d-421b-9d40-1e7eb9b9d36e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:57:50.436299Z","iopub.execute_input":"2025-07-05T16:57:50.436842Z","iopub.status.idle":"2025-07-05T16:57:50.442991Z","shell.execute_reply.started":"2025-07-05T16:57:50.436822Z","shell.execute_reply":"2025-07-05T16:57:50.442329Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def hz_to_bin(f):\n    mask = np.where(f==0.0)\n    \n    cent = 1200 * np.log2((f / 10) + 1e-9)\n    cent -= (1997.37 - 20)\n    cent[mask] = 0.0\n    bin_ = np.floor(cent / 20)\n\n    return np.minimum(bin_, 300)","metadata":{"_uuid":"234a108f-536d-41ca-9acb-5db4e32af58d","_cell_guid":"c567033e-004a-4cec-a0f3-9fc3b457ffce","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:57:53.522024Z","iopub.execute_input":"2025-07-05T16:57:53.522483Z","iopub.status.idle":"2025-07-05T16:57:53.526409Z","shell.execute_reply.started":"2025-07-05T16:57:53.522462Z","shell.execute_reply":"2025-07-05T16:57:53.525668Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def collate_fn(batch, device=TRAINING_DEVICE):\n    feats, labels = [], []\n    for gd_path, lab_path in batch:\n        gd   = np.load(gd_path)         # (1, T, 512)\n        if gd.ndim == 3 and gd.shape[0] == 1 and gd.shape[2] == 512:\n            # feats.append(torch.tensor(gd.squeeze(0), dtype=torch.float32))   # (T,512)\n            # labels.append(torch.tensor(hz_to_bin(np.load(lab_path)), dtype=torch.long))\n            feats.append(torch.tensor(gd.squeeze(0), dtype=torch.float32))   # (T,512)\n            labels.append(torch.tensor(hz_to_bin(np.load(lab_path)), dtype=torch.long))\n\n    if len(feats) == 0:\n        raise RuntimeError(\"All items in batch were invalid.\")\n\n    feats  = pad_sequence(feats,  padding_value=PAD_IDX, batch_first=False)  # (Tmax,B,512)\n    labels = pad_sequence(labels, padding_value=PAD_IDX, batch_first=False)  # (Tmax,B)\n\n    feats  = feats.permute(1,0,2).unsqueeze(1)   # (B,1,T,512)\n    labels = labels.permute(1,0)                                 # (B,Tmax)\n\n    return feats.to(device), labels.to(device)","metadata":{"_uuid":"ead42934-bd3f-4a26-bcc8-72348c9056dc","_cell_guid":"6b56dad2-947d-499f-90ef-fb7938275edd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T17:00:19.984994Z","iopub.execute_input":"2025-07-05T17:00:19.985584Z","iopub.status.idle":"2025-07-05T17:00:19.99113Z","shell.execute_reply.started":"2025-07-05T17:00:19.985564Z","shell.execute_reply":"2025-07-05T17:00:19.990389Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,shuffle=True,  collate_fn=collate_fn)\nvalid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE,shuffle=False, collate_fn=collate_fn)","metadata":{"_uuid":"b8d9c317-bf81-485b-bb0c-44aee95ef202","_cell_guid":"797224cc-09bd-4397-9f5c-a6e692096307","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T17:00:22.053342Z","iopub.execute_input":"2025-07-05T17:00:22.053948Z","iopub.status.idle":"2025-07-05T17:00:22.057631Z","shell.execute_reply.started":"2025-07-05T17:00:22.053897Z","shell.execute_reply":"2025-07-05T17:00:22.056862Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"for x, y in train_loader:\n    print(\"Input shape:\", x.shape)   # e.g., (B, 3, T, 512)\n    print(\"Label shape:\", y.shape)   # e.g., (B, T)\n    break  # Just inspect the first batch","metadata":{"_uuid":"72fa3274-49bb-4972-a73d-19c86b609546","_cell_guid":"e14e5e7c-27db-4d38-8770-4cd24df7f496","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T17:00:23.403038Z","iopub.execute_input":"2025-07-05T17:00:23.403314Z","iopub.status.idle":"2025-07-05T17:00:24.160778Z","shell.execute_reply.started":"2025-07-05T17:00:23.403294Z","shell.execute_reply":"2025-07-05T17:00:24.160064Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Input shape: torch.Size([16, 1, 886, 512])\nLabel shape: torch.Size([16, 886])\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"Model architecture","metadata":{"_uuid":"c8300883-e271-4428-8c6b-5e21faf71bd7","_cell_guid":"8de1ad1e-29d4-44ae-9092-2796e5fc2ec7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class ConvBlockRes(nn.Module):\n    def __init__(self, in_channels, out_channels, momentum=0.01):\n        super(ConvBlockRes, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels,\n                      out_channels=out_channels,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=(1, 1),\n                      bias=False),\n            nn.BatchNorm2d(out_channels, momentum=momentum),\n            nn.ReLU(),\n\n            nn.Conv2d(in_channels=out_channels,\n                      out_channels=out_channels,\n                      kernel_size=(3, 3),\n                      stride=(1, 1),\n                      padding=(1, 1),\n                      bias=False),\n            nn.BatchNorm2d(out_channels, momentum=momentum),\n            nn.ReLU(),\n        )\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv2d(in_channels, out_channels, (1, 1))\n            self.is_shortcut = True\n        else:\n            self.is_shortcut = False\n\n    def forward(self, x):\n        if self.is_shortcut:\n            return self.conv(x) + self.shortcut(x)\n        else:\n            return self.conv(x) + x","metadata":{"_uuid":"f9495db0-44f1-4d29-8e68-989cee4331d6","_cell_guid":"ffe95089-7852-481d-ad48-ec86310d57cf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:58:05.15809Z","iopub.execute_input":"2025-07-05T16:58:05.158577Z","iopub.status.idle":"2025-07-05T16:58:05.164718Z","shell.execute_reply.started":"2025-07-05T16:58:05.158553Z","shell.execute_reply":"2025-07-05T16:58:05.164148Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class ResEncoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, n_blocks=1, momentum=0.01):\n        super(ResEncoderBlock, self).__init__()\n        self.n_blocks = n_blocks\n        self.conv = nn.ModuleList()\n        self.conv.append(ConvBlockRes(in_channels, out_channels, momentum))\n        for i in range(n_blocks - 1):\n            self.conv.append(ConvBlockRes(out_channels, out_channels, momentum))\n        self.kernel_size = kernel_size\n        if self.kernel_size is not None:\n            self.pool = nn.AvgPool2d(kernel_size=kernel_size)\n\n    def forward(self, x):\n        for i in range(self.n_blocks):\n            x = self.conv[i](x)\n        if self.kernel_size is not None:\n            return x, self.pool(x)\n        else:\n            return x","metadata":{"_uuid":"cfbbc803-82f1-4e70-bba1-54d3029807a3","_cell_guid":"eda105d8-284b-4ad1-837a-334e0e2d299a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:58:06.174317Z","iopub.execute_input":"2025-07-05T16:58:06.174938Z","iopub.status.idle":"2025-07-05T16:58:06.180256Z","shell.execute_reply.started":"2025-07-05T16:58:06.174889Z","shell.execute_reply":"2025-07-05T16:58:06.179467Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class ResDecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, n_blocks=1, momentum=0.01):\n        super(ResDecoderBlock, self).__init__()\n        out_padding = (0, 1) if stride == (1, 2) else (1, 1)\n        self.n_blocks = n_blocks\n        self.conv1 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=in_channels,\n                               out_channels=out_channels,\n                               kernel_size=(3, 3),\n                               stride=stride,\n                               padding=(1, 1),\n                               output_padding=out_padding,\n                               bias=False),\n            nn.BatchNorm2d(out_channels, momentum=momentum),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.ModuleList()\n        self.conv2.append(ConvBlockRes(out_channels * 2, out_channels, momentum))\n        for i in range(n_blocks-1):\n            self.conv2.append(ConvBlockRes(out_channels, out_channels, momentum))\n\n    def forward(self, x, concat_tensor):\n        x = self.conv1(x)\n        if x.shape[2:] != concat_tensor.shape[2:]:\n            min_t = min(x.size(2), concat_tensor.size(2))\n            min_f = min(x.size(3), concat_tensor.size(3))\n            x = x[:, :, :min_t, :min_f]\n            concat_tensor = concat_tensor[:, :, :min_t, :min_f]\n        x = torch.cat((x, concat_tensor), dim=1)\n        for i in range(self.n_blocks):\n            x = self.conv2[i](x)\n        return x","metadata":{"_uuid":"940634f8-3a84-4dde-a7c5-4106039ec55a","_cell_guid":"5008cdf4-322d-4321-a44f-0252de6dd12b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:58:07.406354Z","iopub.execute_input":"2025-07-05T16:58:07.406642Z","iopub.status.idle":"2025-07-05T16:58:07.413708Z","shell.execute_reply.started":"2025-07-05T16:58:07.406621Z","shell.execute_reply":"2025-07-05T16:58:07.412946Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"## ENCODER, DECODER, TIMBRE_FILTER AND INTERMEDIATE\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels, in_size, n_encoders, kernel_size, n_blocks, out_channels=16, momentum=0.01):\n        super(Encoder, self).__init__()\n        self.n_encoders = n_encoders\n        self.bn = nn.BatchNorm2d(in_channels, momentum=momentum)\n        self.layers = nn.ModuleList()\n        self.latent_channels = []\n        for i in range(self.n_encoders):\n            self.layers.append(ResEncoderBlock(in_channels, out_channels, kernel_size, n_blocks, momentum=momentum))\n            self.latent_channels.append([out_channels, in_size])\n            in_channels = out_channels\n            out_channels *= 2\n            in_size //= 2\n        self.out_size = in_size\n        self.out_channel = out_channels\n\n    def forward(self, x):\n        concat_tensors = []\n        x = self.bn(x)\n        for i in range(self.n_encoders):\n            _, x = self.layers[i](x)\n            concat_tensors.append(_)\n        return x, concat_tensors\n\nclass Intermediate(nn.Module):\n    def __init__(self, in_channels, out_channels, n_inters, n_blocks, momentum=0.01):\n        super(Intermediate, self).__init__()\n        self.n_inters = n_inters\n        self.layers = nn.ModuleList()\n        self.layers.append(ResEncoderBlock(in_channels, out_channels, None, n_blocks, momentum))\n        for i in range(self.n_inters-1):\n            self.layers.append(ResEncoderBlock(out_channels, out_channels, None, n_blocks, momentum))\n\n    def forward(self, x):\n        for i in range(self.n_inters):\n            x = self.layers[i](x)\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, in_channels, n_decoders, stride, n_blocks, momentum=0.01):\n        super(Decoder, self).__init__()\n        self.layers = nn.ModuleList()\n        self.n_decoders = n_decoders\n        for i in range(self.n_decoders):\n            out_channels = in_channels // 2\n            self.layers.append(ResDecoderBlock(in_channels, out_channels, stride, n_blocks, momentum))\n            in_channels = out_channels\n\n    def forward(self, x, concat_tensors):\n        for i in range(self.n_decoders):\n            x = self.layers[i](x, concat_tensors[-1-i])\n        return x\n\nclass TimbreFilter(nn.Module):\n    def __init__(self, latent_rep_channels):\n        super(TimbreFilter, self).__init__()\n        self.layers = nn.ModuleList()\n        for latent_rep in latent_rep_channels:\n            self.layers.append(ConvBlockRes(latent_rep[0], latent_rep[0]))\n\n    def forward(self, x_tensors):\n        out_tensors = []\n        for i, layer in enumerate(self.layers):\n            out_tensors.append(layer(x_tensors[i]))\n        return out_tensors","metadata":{"_uuid":"abb23bab-f712-4d7d-8bbb-3ed51d07735b","_cell_guid":"de3b0427-4935-4fe5-9608-78fb2f8a897e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:58:09.576845Z","iopub.execute_input":"2025-07-05T16:58:09.577152Z","iopub.status.idle":"2025-07-05T16:58:09.58716Z","shell.execute_reply.started":"2025-07-05T16:58:09.577128Z","shell.execute_reply":"2025-07-05T16:58:09.586515Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class DeepUnet(nn.Module):\n    def __init__(self, kernel_size, n_blocks, en_de_layers=5, inter_layers=4, in_channels=1, en_out_channels=16):\n        super(DeepUnet, self).__init__()\n        self.encoder = Encoder(in_channels, 512, en_de_layers, kernel_size, n_blocks, en_out_channels)\n        self.intermediate = Intermediate(self.encoder.out_channel // 2, self.encoder.out_channel, inter_layers, n_blocks)\n        self.tf = TimbreFilter(self.encoder.latent_channels)\n        self.decoder = Decoder(self.encoder.out_channel, en_de_layers, kernel_size, n_blocks)\n\n    def forward(self, x):\n        x, concat_tensors = self.encoder(x)\n        x = self.intermediate(x)\n        concat_tensors = self.tf(concat_tensors)\n        x = self.decoder(x, concat_tensors)\n        return x\n\nclass BiGRU(nn.Module):\n    def __init__(self, input_features, hidden_features, num_layers):\n        super(BiGRU, self).__init__()\n        self.gru = nn.GRU(input_features, hidden_features, num_layers=num_layers, batch_first=True, bidirectional=True)\n\n    def forward(self, x):\n        self.gru.flatten_parameters()\n        return self.gru(x)[0]","metadata":{"_uuid":"2ab8ba00-1319-46e2-a77b-a1a1d9f49c91","_cell_guid":"e31728b6-d1b1-4e38-af92-a58c1ef727bf","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:58:10.951963Z","iopub.execute_input":"2025-07-05T16:58:10.952511Z","iopub.status.idle":"2025-07-05T16:58:10.958548Z","shell.execute_reply.started":"2025-07-05T16:58:10.952489Z","shell.execute_reply":"2025-07-05T16:58:10.957766Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"## E2E Class\n\nclass E2E(nn.Module):\n    def __init__(self,\n                 n_blocks, n_gru, kernel_size,\n                 en_de_layers=5, inter_layers=4,\n                 in_channels=1,          # 1 → RRCGD channels\n                 en_out_channels=16):\n        super().__init__()\n        self.unet = DeepUnet(kernel_size, n_blocks,\n                             en_de_layers, inter_layers,\n                             in_channels, en_out_channels)\n\n        self.cnn = nn.Conv2d(en_out_channels, 1,kernel_size=(3, 3), padding=(1, 1))\n\n        feat_dim = 512                  # 512 RRCGD features\n        if n_gru:\n            self.fc = nn.Sequential(\n                BiGRU(feat_dim, 256, n_gru),\n                nn.Linear(512, N_CLASS),\n                nn.Dropout(0.25),\n                nn.Sigmoid()\n            )\n        else:\n            self.fc = nn.Sequential(\n                nn.Linear(feat_dim, N_CLASS),\n                nn.Dropout(0.25),\n                nn.Sigmoid()\n            )\n\n    def forward(self, feats):             # feats : (B, 1, T, 512)\n        x = self.cnn(self.unet(feats))    # (B, 1, T, 512)\n        x = x.transpose(1, 2).flatten(-2) # (B, T, 512)  ← now matches GRU\n\n        hidden_vec = None\n        for i, layer in enumerate(self.fc):\n            x = layer(x)\n            if i == 0:                    # BiGRU output\n                hidden_vec = x            # (B, T, 512)\n\n        return hidden_vec, x              # (B, T, 512)","metadata":{"_uuid":"75cdd958-e9a6-4f03-912f-29a075a70423","_cell_guid":"834877c8-c76f-436e-b672-ec63fcb9bdc7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:58:12.397366Z","iopub.execute_input":"2025-07-05T16:58:12.397623Z","iopub.status.idle":"2025-07-05T16:58:12.403831Z","shell.execute_reply.started":"2025-07-05T16:58:12.397605Z","shell.execute_reply":"2025-07-05T16:58:12.403193Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def train_and_eval(model, train_loader, valid_loader, optimizer, criterion, max_steps, TRAINING_DEVICE, dump_dir, model_name, metric, last_epoch=0):\n    \n    checkpint_store = ModelCheckpointStore(dump_dir)\n    \n    training_metrics = {\"train_avg_loss\":[],\n                        \"train_avg_acc\":[],\n                        \"valid_avg_loss\":[],\n                        \"valid_avg_acc\":[]}\n    for step in range(last_epoch, max_steps+last_epoch):\n        train_step_loss = 0\n        train_step_correct_pred = 0\n        valid_step_loss = 0\n        valid_step_correct_pred = 0\n        model.train()\n        for batch_idx, (x, y) in enumerate(train_loader):\n            print(f\"At {batch_idx} in training loop\")\n            # x = torch.transpose(x, 0, 1)\n            # y = torch.transpose(y, 0, 1)\n            x, y = x.to(TRAINING_DEVICE), y.to(TRAINING_DEVICE)\n\n            optimizer.zero_grad()\n            _,train_out = model(x)\n            # print(\"train_out shape:\", train_out.shape)\n            # print(\"y shape:\", y.shape)\n            # train_out:  (B, T_feat, 360)\n            # y         : (B, T_lab)\n            T = train_out.size(1)\n            if y.size(1) != T:          # crop labels if longer\n                y = y[:, :T]\n            # print('new shape:',y.shape)\n\n            loss = criterion(train_out.reshape(-1, N_CLASS), y.flatten())\n            loss.backward()\n            optimizer.step()\n            train_step_loss += loss.item()\n            _, train_correct_outputs = torch.max(train_out.reshape(-1, N_CLASS), dim=1)\n            train_step_correct_pred += (train_correct_outputs == y.flatten()).sum().item()\n\n        training_metrics[\"train_avg_loss\"].append(train_step_loss / len(train_loader))\n        training_metrics[\"train_avg_acc\"].append(train_step_correct_pred / len(train_loader))\n        \n        model.eval()\n        with torch.no_grad():\n            for batch_idx, (x_val, y_val) in enumerate(valid_loader):\n                print(f\"At {batch_idx} in validation loop\")\n\n                x_val, y_val = x_val.to(TRAINING_DEVICE), y_val.to(TRAINING_DEVICE)\n                _, valid_out = model(x_val)           # (B, T_feat, N_CLASS)\n\n                # ---------- align lengths ------------\n                T = min(valid_out.size(1), y_val.size(1))\n                valid_out = valid_out[:, :T, :]       # <-- fixed\n                y_val     = y_val[:, :T]\n                # -------------------------------------\n\n                # loss & accuracy\n                valid_loss = criterion(valid_out.reshape(-1, N_CLASS), y_val.flatten())\n                valid_step_loss += valid_loss.item()\n\n                _, valid_pred = torch.max(valid_out.reshape(-1, N_CLASS), dim=1)\n                valid_step_correct_pred += (valid_pred == y_val.flatten()).sum().item()\n\n            training_metrics[\"valid_avg_loss\"].append(valid_step_loss / len(valid_loader))\n            training_metrics[\"valid_avg_acc\"].append(valid_step_correct_pred / len(valid_loader))\n        torch.cuda.empty_cache()\n        display(step, training_metrics)\n        checkpint_store(model, training_metrics, metric, step)\n        plot(dump_dir, model_name, **training_metrics)","metadata":{"_uuid":"aff63db1-2030-4dd1-a3ee-28f6d7d77607","_cell_guid":"68532240-e449-46e9-808f-e6960f106db5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:58:16.495398Z","iopub.execute_input":"2025-07-05T16:58:16.495674Z","iopub.status.idle":"2025-07-05T16:58:16.505247Z","shell.execute_reply.started":"2025-07-05T16:58:16.495656Z","shell.execute_reply":"2025-07-05T16:58:16.504567Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\nimport gc\n\n# Clear unused memory\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"_uuid":"71bd82f6-a6bd-4750-b9a7-86c9b8d6056d","_cell_guid":"ccb11d8b-be20-4bf5-91ae-f6e20746302a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-03T10:59:22.602292Z","iopub.execute_input":"2025-07-03T10:59:22.602568Z","iopub.status.idle":"2025-07-03T10:59:22.811378Z","shell.execute_reply.started":"2025-07-03T10:59:22.602549Z","shell.execute_reply":"2025-07-03T10:59:22.81072Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"_uuid":"800447a4-a619-469d-8e1b-a12b18605f9c","_cell_guid":"206ed3cd-4cd4-4172-bba8-d9fa62809f0d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:58:23.524289Z","iopub.execute_input":"2025-07-05T16:58:23.524566Z","iopub.status.idle":"2025-07-05T16:58:23.528313Z","shell.execute_reply.started":"2025-07-05T16:58:23.524544Z","shell.execute_reply":"2025-07-05T16:58:23.527656Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print(\"After clearing:\")\nt = torch.cuda.get_device_properties(0).total_memory\nr = torch.cuda.memory_reserved(0)\na = torch.cuda.memory_allocated(0)\nf = r - a\nprint(f\"Total: {t}, Reserved: {r}, Allocated: {a}, Free inside reserved: {f}\")","metadata":{"_uuid":"0002d4d7-c408-430c-84ec-1977cd63b7a2","_cell_guid":"3fb7d7cb-dad6-40c5-ae34-ba2c557738b9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T17:00:32.114892Z","iopub.execute_input":"2025-07-05T17:00:32.115725Z","iopub.status.idle":"2025-07-05T17:00:32.120289Z","shell.execute_reply.started":"2025-07-05T17:00:32.115697Z","shell.execute_reply":"2025-07-05T17:00:32.119652Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"After clearing:\nTotal: 17059545088, Reserved: 517996544, Allocated: 451589632, Free inside reserved: 66406912\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# DEFINING MODEL AND PARAMETER SIZE\nmodel = nn.DataParallel(E2E(n_blocks=4,n_gru=1,kernel_size=(2, 2),en_de_layers=5,inter_layers=4,in_channels=1)).to(TRAINING_DEVICE)","metadata":{"_uuid":"2bdb0abb-6b64-4e87-ad58-c1ddcf6e3bca","_cell_guid":"59e7ac39-37e5-496b-9152-e0ab31c26ce7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:59:16.221794Z","iopub.execute_input":"2025-07-05T16:59:16.222114Z","iopub.status.idle":"2025-07-05T16:59:17.193115Z","shell.execute_reply.started":"2025-07-05T16:59:16.222093Z","shell.execute_reply":"2025-07-05T16:59:17.192551Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Parameter Count: {count_parameters(model)}\")","metadata":{"_uuid":"97225197-f256-4065-88cd-b8c365581d9e","_cell_guid":"07a2f8fe-a1b7-45ab-ae45-dcfc861288b0","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:59:20.300458Z","iopub.execute_input":"2025-07-05T16:59:20.301151Z","iopub.status.idle":"2025-07-05T16:59:20.306751Z","shell.execute_reply.started":"2025-07-05T16:59:20.301127Z","shell.execute_reply":"2025-07-05T16:59:20.305961Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Parameter Count: 92192795\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"cross_entropy_loss_weights = torch.full((N_CLASS,), 1.5)\ncross_entropy_loss_weights[0] = 1\ncross_entropy_loss_weights = cross_entropy_loss_weights.float().to(TRAINING_DEVICE)","metadata":{"_uuid":"44ecb612-89ad-44cd-b3cb-7848bac5c889","_cell_guid":"6eae10c6-2c76-49e2-8e6f-0c4e9b74f7b5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:59:22.565475Z","iopub.execute_input":"2025-07-05T16:59:22.566082Z","iopub.status.idle":"2025-07-05T16:59:22.570406Z","shell.execute_reply.started":"2025-07-05T16:59:22.56606Z","shell.execute_reply":"2025-07-05T16:59:22.569691Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss(weight=cross_entropy_loss_weights)\noptimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)","metadata":{"_uuid":"c127e940-1d77-44f6-8d61-5a6f4df8c2a3","_cell_guid":"d9d1d139-156f-4f22-a254-4bb2c6484422","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:59:23.743138Z","iopub.execute_input":"2025-07-05T16:59:23.74342Z","iopub.status.idle":"2025-07-05T16:59:23.748943Z","shell.execute_reply.started":"2025-07-05T16:59:23.7434Z","shell.execute_reply":"2025-07-05T16:59:23.74829Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# dump_dir = \"/speech/nishanth/clean_research/nishchala/ptdb_full_data/mudit_4_layer_pos_gd_1.008\"\ndump_dir = \"/kaggle/working/pitch-estimate/trail1\"\nos.makedirs(dump_dir, exist_ok=True)","metadata":{"_uuid":"8960032f-e106-44e1-9442-4823a3514f7b","_cell_guid":"9a355a31-e99a-41f4-ae14-0782f9885d8f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:59:25.124818Z","iopub.execute_input":"2025-07-05T16:59:25.12552Z","iopub.status.idle":"2025-07-05T16:59:25.12905Z","shell.execute_reply.started":"2025-07-05T16:59:25.125496Z","shell.execute_reply":"2025-07-05T16:59:25.128413Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"last_model_save_epoch = 0\nmodel_name = \"new_rrcgd_rmvpe_1\"\nepochs = 30","metadata":{"_uuid":"3516a5a8-3b19-4736-b516-404740fe4e9b","_cell_guid":"b3c160ec-3944-4820-a50f-46f0ba3043fc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:59:26.574069Z","iopub.execute_input":"2025-07-05T16:59:26.575012Z","iopub.status.idle":"2025-07-05T16:59:26.57816Z","shell.execute_reply.started":"2025-07-05T16:59:26.574984Z","shell.execute_reply":"2025-07-05T16:59:26.577546Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\nprint(f\"Reserved:  {torch.cuda.memory_reserved() / 1e9:.2f} GB\")","metadata":{"_uuid":"3779ad55-333b-43d9-8279-1524ee02df2e","_cell_guid":"76bcccd6-90f3-4eca-936e-6980df8c8ca9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T16:59:28.390443Z","iopub.execute_input":"2025-07-05T16:59:28.390958Z","iopub.status.idle":"2025-07-05T16:59:28.395456Z","shell.execute_reply.started":"2025-07-05T16:59:28.390931Z","shell.execute_reply":"2025-07-05T16:59:28.394767Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Allocated: 0.39 GB\nReserved:  0.44 GB\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"train_and_eval(model, train_loader, valid_loader, optimizer, loss_fn, epochs, TRAINING_DEVICE, dump_dir, model_name, \"valid_avg_loss\", last_epoch=0)","metadata":{"_uuid":"f935a45d-b517-481b-984f-860fd13adbf2","_cell_guid":"121e4d7e-e921-40b8-ba7d-179d92b3c4a8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-05T17:00:39.736643Z","iopub.execute_input":"2025-07-05T17:00:39.737328Z","iopub.status.idle":"2025-07-05T17:00:41.345534Z","shell.execute_reply.started":"2025-07-05T17:00:39.737306Z","shell.execute_reply":"2025-07-05T17:00:41.344508Z"},"scrolled":true,"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"At 0 in training loop\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/318110147.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINING_DEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valid_avg_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/215296306.py\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, train_loader, valid_loader, optimizer, criterion, max_steps, TRAINING_DEVICE, dump_dir, model_name, metric, last_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;31m# print(\"train_out shape:\", train_out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# print(\"y shape:\", y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/1028953624.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feats)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m             \u001b[0;31m# feats : (B, 1, T, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# (B, 1, T, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, 512)  ← now matches GRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/977312740.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mconcat_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/208053776.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_encoders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mconcat_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2008798577.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3407528873.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 52.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 27.12 MiB is free. Process 3470 has 15.86 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 130.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 52.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 27.12 MiB is free. Process 3470 has 15.86 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 130.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"_uuid":"482312c5-54bb-4df5-98c0-6a4b5b93320b","_cell_guid":"f781bd73-b524-4301-adab-428c0d81407d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}